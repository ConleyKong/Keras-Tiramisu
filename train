#!/usr/bin/python

import argparse
import os
import pickle

from keras import optimizers
from keras.callbacks import ModelCheckpoint
from keras.callbacks import TensorBoard
from keras.utils import plot_model


import model
from utils import MapillaryGenerator, Visualization, make_parallel


parser = argparse.ArgumentParser()
parser.add_argument('--batch_size', type=int, default=3, help='input batch size')
parser.add_argument('--image_width', type=int, default=640, help='the input image width')
parser.add_argument('--image_height', type=int, default=360, help='the input image height')
parser.add_argument('--crop_width', type=int, default=224, help='the crop width')
parser.add_argument('--crop_height', type=int, default=224, help='the crop height')
parser.add_argument('--random_flip', type=bool, default=True, help='Do random horizontal flip during training')
parser.add_argument('--epochs', type=int, default=100, help='number of epochs to train for')
parser.add_argument('--lr', type=float, default=1e-4, help='learning rate of the optimizer')
parser.add_argument('--decay', type=float, default=1e-4, help='learning rate decay (per batch update)')
parser.add_argument('--n_gpu', type=int, default=1, help='Number of GPUs to use')
parser.add_argument('--n_cpu', type=int, default=8, help='Number of CPU threads to use during data generation')
parser.add_argument('--weights', type=str, default='weights.h5', help='output path for the weights file')
opt = parser.parse_args()

print(opt)

# Callbacks
checkpoint = ModelCheckpoint(opt.weights, save_best_only=True, save_weights_only=True, verbose=1)
tensorboard = TensorBoard(batch_size=opt.batch_size)
visualization = Visualization(resize_shape=(opt.crop_width, opt.crop_height), batch_steps=10, n_gpu=opt.n_gpu)

# Load class weights
class_weights = pickle.load(open("class_weights.p", "rb")) # Not usable for now :(

# Batch size has to be incremented by number of GPUs (each GPU processes batch_size samples)
opt.batch_size *= opt.n_gpu 

#### Train initially with random crops ####

# Generators
train_generator = MapillaryGenerator(batch_size=opt.batch_size, crop_shape=(opt.crop_width, opt.crop_height), resize_shape=(opt.image_width, opt.image_height), horizontal_flip=opt.random_flip)
val_generator = MapillaryGenerator(mode='validation', batch_size=opt.batch_size, crop_shape=(opt.crop_width, opt.crop_height), resize_shape=(opt.image_width, opt.image_height), horizontal_flip=opt.random_flip)

optim = optimizers.Adam(lr=opt.lr, decay=opt.decay)
tiramisu = model.build(opt.crop_width, opt.crop_height, 66)
tiramisu = make_parallel(tiramisu, opt.n_gpu)
tiramisu.compile(optim, 'categorical_crossentropy', metrics=['categorical_accuracy'])
tiramisu.fit_generator(train_generator, len(train_generator), opt.epochs, callbacks=[checkpoint, tensorboard, visualization], 
                       validation_data=val_generator, validation_steps=len(val_generator), workers=opt.n_cpu)
                       
#### Finetune with full size image and lower learning rate ####
                       
# Generators
train_generator = MapillaryGenerator(batch_size=opt.batch_size, crop_shape=None, resize_shape=(opt.image_width, opt.image_height), horizontal_flip=opt.random_flip)
val_generator = MapillaryGenerator(mode='validation', batch_size=opt.batch_size, crop_shape=None, resize_shape=(opt.image_width, opt.image_height), horizontal_flip=opt.random_flip)                       

optim = optimizers.Adam(lr=opt.lr/10)
tiramisu = model.build(opt.image_width, opt.image_height, 66, weights_path=opt.weights)
tiramisu = make_parallel(tiramisu, opt.n_gpu)
tiramisu.compile(optim, 'categorical_crossentropy', metrics=['categorical_accuracy'])
tiramisu.fit_generator(train_generator, len(train_generator), opt.epochs, callbacks=[checkpoint, tensorboard, visualization], 
                       validation_data=val_generator, validation_steps=len(val_generator), workers=opt.n_cpu)
